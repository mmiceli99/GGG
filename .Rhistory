test <- vroom('test.csv')
missing <- vroom('trainWithMissingValues.csv')
my_recipe <- recipe(type ~ ., data=missing) %>%
step_impute_mean(hair_length) %>%
step_impute_mean(rotting_flesh) %>%
step_impute_mean(bone_length) %>%
prep <- prep(my_recipe)
my_recipe <- recipe(type ~ ., data=missing) %>%
step_impute_mean(hair_length) %>%
step_impute_mean(rotting_flesh) %>%
step_impute_mean(bone_length)
prep <- prep(my_recipe)
baked <- bake(prep, new_data = train)
rmse_vec(train[is.na(missing)], baked[is.na[missing]])
View(baked)
View(baked)
View(missing)
View(baked)
rmse_vec(train[is.na(missing)], baked[is.na(missing)])
baked[is.na(missing)]
train[is.na(missing)]
rmse_vec(train[is.na(missing)], baked)
rmse_vec(train[is.na(missing)], baked[is.na(missing)])
is.na(missing)
baked <- bake(prep, new_data = missing)
rmse_vec(train[is.na(missing)], baked[is.na(missing)])
#impute_knn
my_recipe <- recipe(type ~ ., data=missing) %>%
step_impute_knn(hair_length, k=5) %>%
step_impute_knn(rotting_flesh, k=5) %>%
step_impute_knn(bone_length, k=5)
prep <- prep(my_recipe)
baked <- bake(prep, new_data = missing)
#impute_knn
my_recipe <- recipe(type ~ ., data=missing) %>%
step_impute_knn(hair_length, k=5) %>%
step_impute_knn(rotting_flesh, k=5) %>%
step_impute_knn(bone_length, k=5)
prep <- prep(my_recipe)
#impute_knn
my_recipe <- recipe(type ~ ., data=missing) %>%
step_impute_bag(hair_length,  trees=5) %>%
step_impute_bag(rotting_flesh, trees=5) %>%
step_impute_bag(bone_length,  trees=5)
prep <- prep(my_recipe)
baked <- bake(prep, new_data = missing)
rmse_vec(train[is.na(missing)], baked[is.na(missing)])
#impute_knn
my_recipe <- recipe(type ~ ., data=missing) %>%
step_impute_bag(hair_length,  trees=100) %>%
step_impute_bag(rotting_flesh, trees=100) %>%
step_impute_bag(bone_length,  trees=100)
prep <- prep(my_recipe)
baked <- bake(prep, new_data = missing)
rmse_vec(train[is.na(missing)], baked[is.na(missing)])
#impute_knn
my_recipe <- recipe(type ~ ., data=missing) %>%
step_impute_bag(hair_length,  trees=1000) %>%
step_impute_bag(rotting_flesh, trees=1000) %>%
step_impute_bag(bone_length,  trees=1000)
prep <- prep(my_recipe)
baked <- bake(prep, new_data = missing)
rmse_vec(train[is.na(missing)], baked[is.na(missing)])
is.na(missing)
train[is.na(missing)]
baked[is.na(missing)]
library(vroom)
library(tidyverse)
library(tidymodels)
library(embed)
library(doParallel)
library(themis)
#parallel::detectCores() #How many cores do I have?
cl <- makePSOCKcluster(13) # num_cores to use
registerDoParallel(cl)
train <- vroom("./train.csv")
test <- vroom("./test.csv")
train <- train %>%
mutate(type = as.factor(type))
my_recipe <- recipe(type ~ ., data=train) %>%
update_role(id, new_role = "id variable") %>%
step_mutate_at(color, fn = factor) %>%# turn color into factors
step_lencode_glm(color, outcome=vars(type))
#step_other(all_nominal_predictors(), threshold = .001) %>% # combines categorical values that occur <5% into an "other" value
#step_dummy(color) # dummy variable encoding
#step_lencode_mixed(color, outcome = vars(type)) #%>% #target encoding
#step_smote(all_outcomes(), k=2)
# also step_lencode_glm() and step_lencode_bayes()
# NOTE: some of these step functions are not appropriate to use together
# apply the recipe to your data
prep <- prep(my_recipe)
baked <- bake(prep, new_data = train)
my_mod <- rand_forest(mtry = tune(),
min_n=tune(),
trees=1000) %>%
set_engine("ranger") %>%
set_mode("classification")
GGG_workflow <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(my_mod)
## Grid of values to tune over
tuning_grid <- grid_regular(mtry(range=c(1,10)),
min_n(),
levels = 5) ## L^2 total tuning possibilities
## Split data for CV
folds <- vfold_cv(train, v = 5, repeats=1)
## Run the CV
CV_results <- GGG_workflow %>%
tune_grid(resamples=folds,
grid=tuning_grid,
metrics=metric_set(accuracy)) #Or leave metrics NULL
## Find Best Tuning Parameters
bestTune <- CV_results %>%
select_best()
## Finalize the Workflow & fit it
final_wf <-
GGG_workflow %>%
finalize_workflow(bestTune) %>%
fit(data=train)
## Predict
ama_predictions <- predict(final_wf, new_data=test, type='class') %>%
rename(type=.pred_class) %>%
mutate(id = test$id) %>%
select(id, type)
vroom_write(x=ama_predictions, file="./RandFor.csv", delim=",")
stopCluster(cl)
install.packages('keras')
library(vroom)
library(tidyverse)
library(tidymodels)
library(embed)
library(doParallel)
library(themis)
library(keras)
#parallel::detectCores() #How many cores do I have?
cl <- makePSOCKcluster(13) # num_cores to use
registerDoParallel(cl)
train <- vroom("./train.csv")
test <- vroom("./test.csv")
train <- train %>%
mutate(type = as.factor(type))
nn_recipe <- recipe(formula=, data=) %>%
update_role(id, new_role="id variable") %>%
step_mutate_at(color, fn = factor) %>%# turn color into factors
step_lencode_glm(color, outcome=vars(type)) %>%
step_range(all_numeric_predictors(), min=0, max=1) #scale to [0,1]
stopCluster(cl)
nn_recipe <- recipe(formula=, data=) %>%
update_role(id, new_role="id variable") %>%
step_mutate_at(color, fn = factor) %>%# turn color into factors
step_lencode_glm(color, outcome=vars(type)) %>%
step_range(all_numeric_predictors(), min=0, max=1) #scale to [0,1]
train <- vroom("./train.csv")
test <- vroom("./test.csv")
train <- train %>%
mutate(type = as.factor(type))
nn_recipe <- recipe(formula=, data=) %>%
update_role(id, new_role="id variable") %>%
step_mutate_at(color, fn = factor) %>%# turn color into factors
step_lencode_glm(color, outcome=vars(type)) %>%
step_range(all_numeric_predictors(), min=0, max=1) #scale to [0,1]
nn_recipe <- recipe(formula=, data=) %>%
update_role(id, new_role="id variable") %>%
step_mutate_at(color, fn = factor) %>%# turn color into factors
step_lencode_glm(color, outcome=vars(type))# %>%
train <- vroom("./train.csv")
test <- vroom("./test.csv")
train <- train %>%
mutate(type = as.factor(type))
my_recipe <- recipe(type ~ ., data=train) %>%
update_role(id, new_role = "id variable") %>%
step_mutate_at(color, fn = factor) %>%# turn color into factors
step_lencode_glm(color, outcome=vars(type))
nn_recipe <- recipe(type ~ ., data=train) %>%
update_role(id, new_role = "id variable") %>%
step_mutate_at(color, fn = factor) %>%# turn color into factors
step_lencode_glm(color, outcome=vars(type))
nn_recipe <- recipe(type ~ ., data=train) %>%
update_role(id, new_role = "id variable") %>%
step_mutate_at(color, fn = factor) %>%# turn color into factors
step_lencode_glm(color, outcome=vars(type)) %>%
step_range(all_numeric_predictors(), min=0, max=1) #scale to [0,1]
prep <- prep(my_recipe)
baked <- bake(prep, new_data = train)
nn_model <- mlp(hidden_units = tune(),
epochs = 100, #or 100 or 250
activation="relu") %>%
set_engine("keras", verbose=0) %>% #verbose = 0 prints off less
set_mode("classification")
nn_tuneGrid <- grid_regular(hidden_units(range=c(1, 50)),
levels=tune())
nn_tuneGrid <- grid_regular(hidden_units(range=c(1, maxHiddenUnits))),
nn_tuneGrid <- grid_regular(hidden_units(range=c(1, 50))),
folds <- vfold_cv(ama_train, v = 5, repeats=1)
folds <- vfold_cv(train, v = 5, repeats=1)
nn_tuneGrid <- grid_regular(hidden_units(range=c(1, 50))),
nn_tuneGrid <- grid_regular(hidden_units(range=c(1, 50)),
levels=tune())
train <- vroom("./train.csv")
test <- vroom("./test.csv")
train <- train %>%
mutate(type = as.factor(type))
nn_recipe <- recipe(type ~ ., data=train) %>%
update_role(id, new_role = "id variable") %>%
step_mutate_at(color, fn = factor) %>%# turn color into factors
step_lencode_glm(color, outcome=vars(type)) %>%
step_range(all_numeric_predictors(), min=0, max=1) #scale to [0,1]
prep <- prep(my_recipe)
baked <- bake(prep, new_data = train)
nn_model <- mlp(hidden_units = tune(),
epochs = 100, #or 100 or 250
activation="relu") %>%
set_engine("keras", verbose=0) %>% #verbose = 0 prints off less
set_mode("classification")
nn_tuneGrid <- grid_regular(hidden_units(range=c(1, 50)),
levels=tune())
nn_tuneGrid <- grid_regular(hidden_units(range=c(1, 200)),
levels=300)
folds <- vfold_cv(train, v = 5, repeats=1)
tuned_nn <- nn_wf %>%
tune_grid(resamples=fold,
grid=nn_tuning_grid,
metrics=metric_set(accuracy))
library(vroom)
library(tidyverse)
library(tidymodels)
library(embed)
library(doParallel)
library(themis)
library(keras)
#parallel::detectCores() #How many cores do I have?
cl <- makePSOCKcluster(13) # num_cores to use
registerDoParallel(cl)
train <- vroom("./train.csv")
test <- vroom("./test.csv")
train <- train %>%
mutate(type = as.factor(type))
nn_recipe <- recipe(type ~ ., data=train) %>%
update_role(id, new_role = "id variable") %>%
step_mutate_at(color, fn = factor) %>%# turn color into factors
step_lencode_glm(color, outcome=vars(type)) %>%
step_range(all_numeric_predictors(), min=0, max=1) #scale to [0,1]
prep <- prep(my_recipe)
my_recipe <- recipe(type ~ ., data=train) %>%
update_role(id, new_role = "id variable") %>%
step_mutate_at(color, fn = factor) %>%# turn color into factors
step_lencode_glm(color, outcome=vars(type)) %>%
step_range(all_numeric_predictors(), min=0, max=1) #scale to [0,1]
prep <- prep(my_recipe)
baked <- bake(prep, new_data = train)
nn_model <- mlp(hidden_units = tune(),
epochs = 100, #or 100 or 250
activation="relu") %>%
set_engine("keras", verbose=0) %>% #verbose = 0 prints off less
set_mode("classification")
nn_tuneGrid <- grid_regular(hidden_units(range=c(1, 200)),
levels=300)
folds <- vfold_cv(train, v = 5, repeats=1)
nn_workflow <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(my_mod)
nn_workflow <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(nn_model)
tuned_nn <- nn_workflow %>%
tune_grid(resamples=fold,
grid=nn_tuning_grid,
metrics=metric_set(accuracy))
tuned_nn <- nn_workflow %>%
tune_grid(resamples=fold,
grid=nn_tunegrid,
metrics=metric_set(accuracy))
tuned_nn <- nn_workflow %>%
tune_grid(resamples=fold,
grid=nn_tuneGrid,
metrics=metric_set(accuracy))
tuned_nn <- nn_workflow %>%
tune_grid(resamples=folds,
grid=nn_tuneGrid,
metrics=metric_set(accuracy))
stopCluster(cl)
train <- vroom("./train.csv")
test <- vroom("./test.csv")
train <- train %>%
mutate(type = as.factor(type))
my_recipe <- recipe(type ~ ., data=train) %>%
update_role(id, new_role = "id variable") %>%
step_mutate_at(color, fn = factor) %>%# turn color into factors
step_lencode_glm(color, outcome=vars(type)) %>%
step_range(all_numeric_predictors(), min=0, max=1) #scale to [0,1]
prep <- prep(my_recipe)
baked <- bake(prep, new_data = train)
nn_model <- mlp(hidden_units = tune(),
epochs = 100, #or 100 or 250
activation="relu") %>%
set_engine("keras", verbose=0) %>% #verbose = 0 prints off less
set_mode("classification")
nn_tuneGrid <- grid_regular(hidden_units(range=c(1, 200)),
levels=30)
folds <- vfold_cv(train, v = 5, repeats=1)
nn_workflow <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(nn_model)
tuned_nn <- nn_workflow %>%
tune_grid(resamples=folds,
grid=nn_tuneGrid,
metrics=metric_set(accuracy))
stopCluster(cl)
stopCluster(cl)
train <- vroom("./train.csv")
test <- vroom("./test.csv")
train <- train %>%
mutate(type = as.factor(type))
my_recipe <- recipe(type ~ ., data=train) %>%
update_role(id, new_role = "id variable") %>%
step_mutate_at(color, fn = factor) %>%# turn color into factors
step_lencode_glm(color, outcome=vars(type)) %>%
step_range(all_numeric_predictors(), min=0, max=1) #scale to [0,1]
prep <- prep(my_recipe)
baked <- bake(prep, new_data = train)
nn_model <- mlp(hidden_units = tune(),
epochs = 100, #or 100 or 250
activation="relu") %>%
set_engine("keras", verbose=0) %>% #verbose = 0 prints off less
set_mode("classification")
nn_tuneGrid <- grid_regular(hidden_units(range=c(1, 200)),
levels=30)
folds <- vfold_cv(train, v = 5, repeats=1)
nn_workflow <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(nn_model)
tuned_nn <- nn_workflow %>%
tune_grid(resamples=folds,
grid=nn_tuneGrid,
metrics=metric_set(accuracy))
tuned_nn %>% collect_metrics() %>%
filter(.metric=="accuracy") %>%
ggplot(aes(x=hidden_units, y=mean)) + geom_line()
install_keras()
install_keras()
train <- vroom("./train.csv")
install_tensorflow()
install.packages('tensorflow')
library(tensorflow)
library(vroom)
library(tidyverse)
library(tidymodels)
library(embed)
library(doParallel)
library(themis)
library(keras)
library(tensorflow)
#parallel::detectCores() #How many cores do I have?
#cl <- makePSOCKcluster(13) # num_cores to use
#registerDoParallel(cl)
train <- vroom("./train.csv")
test <- vroom("./test.csv")
train <- train %>%
mutate(type = as.factor(type))
my_recipe <- recipe(type ~ ., data=train) %>%
update_role(id, new_role = "id variable") %>%
step_mutate_at(color, fn = factor) %>%# turn color into factors
step_lencode_glm(color, outcome=vars(type)) %>%
step_range(all_numeric_predictors(), min=0, max=1) #scale to [0,1]
prep <- prep(my_recipe)
baked <- bake(prep, new_data = train)
nn_model <- mlp(hidden_units = tune(),
epochs = 100, #or 100 or 250
activation="relu") %>%
set_engine("keras", verbose=0) %>% #verbose = 0 prints off less
set_mode("classification")
nn_tuneGrid <- grid_regular(hidden_units(range=c(1, 200)),
levels=30)
folds <- vfold_cv(train, v = 5, repeats=1)
nn_workflow <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(nn_model)
tuned_nn <- nn_workflow %>%
tune_grid(resamples=folds,
grid=nn_tuneGrid,
metrics=metric_set(accuracy))
tuned_nn %>% collect_metrics() %>%
filter(.metric=="accuracy") %>%
ggplot(aes(x=hidden_units, y=mean)) + geom_line()
install.packages('nnet')
install.packages("nnet")
library(nnet)
library(vroom)
library(tidyverse)
library(tidymodels)
library(embed)
library(doParallel)
library(themis)
library(nnet)
#parallel::detectCores() #How many cores do I have?
#cl <- makePSOCKcluster(13) # num_cores to use
#registerDoParallel(cl)
train <- vroom("./train.csv")
test <- vroom("./test.csv")
train <- train %>%
mutate(type = as.factor(type))
my_recipe <- recipe(type ~ ., data=train) %>%
update_role(id, new_role = "id variable") %>%
step_mutate_at(color, fn = factor) %>%# turn color into factors
step_lencode_glm(color, outcome=vars(type)) %>%
step_range(all_numeric_predictors(), min=0, max=1) #scale to [0,1]
prep <- prep(my_recipe)
baked <- bake(prep, new_data = train)
nn_model <- mlp(hidden_units = tune(),
epochs = 100) %>%
set_engine("nnet") %>% #verbose = 0 prints off less
set_mode("classification")
nn_tuneGrid <- grid_regular(hidden_units(range=c(1, 200)),
levels=30)
folds <- vfold_cv(train, v = 5, repeats=1)
nn_workflow <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(nn_model)
tuned_nn <- nn_workflow %>%
tune_grid(resamples=folds,
grid=nn_tuneGrid,
metrics=metric_set(accuracy))
tuned_nn %>% collect_metrics() %>%
filter(.metric=="accuracy") %>%
ggplot(aes(x=hidden_units, y=mean)) + geom_line()
## Find Best Tuning Parameters
bestTune <- tuned_nn %>%
select_best()
## Finalize the Workflow & fit it
final_wf <-
nn_workflow %>%
finalize_workflow(bestTune) %>%
fit(data=train)
## Predict
ama_predictions <- predict(final_wf, new_data=test, type='class') %>%
rename(type=.pred_class) %>%
mutate(id = test$id) %>%
select(id, type)
vroom_write(x=ama_predictions, file="./nerualNet.csv", delim=",")
train <- vroom("./train.csv")
test <- vroom("./test.csv")
train <- train %>%
mutate(type = as.factor(type))
my_recipe <- recipe(type ~ ., data=train) %>%
update_role(id, new_role = "id variable") %>%
step_mutate_at(color, fn = factor) %>%# turn color into factors
step_lencode_glm(color, outcome=vars(type)) %>%
step_range(all_numeric_predictors(), min=0, max=1) #scale to [0,1]
prep <- prep(my_recipe)
baked <- bake(prep, new_data = train)
nn_model <- mlp(hidden_units = tune(),
epochs = 100) %>%
set_engine("nnet") %>% #verbose = 0 prints off less
set_mode("classification")
nn_tuneGrid <- grid_regular(hidden_units(range=c(1, 200)),
levels=50)
folds <- vfold_cv(train, v = 5, repeats=1)
nn_workflow <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(nn_model)
tuned_nn <- nn_workflow %>%
tune_grid(resamples=folds,
grid=nn_tuneGrid,
metrics=metric_set(accuracy))
tuned_nn %>% collect_metrics() %>%
filter(.metric=="accuracy") %>%
ggplot(aes(x=hidden_units, y=mean)) + geom_line()
#Or leave metrics NULL
## Find Best Tuning Parameters
bestTune <- tuned_nn %>%
select_best()
## Finalize the Workflow & fit it
final_wf <-
nn_workflow %>%
finalize_workflow(bestTune) %>%
fit(data=train)
## Predict
ama_predictions <- predict(final_wf, new_data=test, type='class') %>%
rename(type=.pred_class) %>%
mutate(id = test$id) %>%
select(id, type)
vroom_write(x=ama_predictions, file="./nerualNet.csv", delim=",")
#stopCluster(cl)
nn_tuneGrid <- grid_regular(hidden_units(range=c(1, 80)),
levels=200)
folds <- vfold_cv(train, v = 5, repeats=1)
nn_workflow <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(nn_model)
tuned_nn <- nn_workflow %>%
tune_grid(resamples=folds,
grid=nn_tuneGrid,
metrics=metric_set(accuracy))
tuned_nn %>% collect_metrics() %>%
filter(.metric=="accuracy") %>%
ggplot(aes(x=hidden_units, y=mean)) + geom_line()
#Or leave metrics NULL
## Find Best Tuning Parameters
bestTune <- tuned_nn %>%
select_best()
## Finalize the Workflow & fit it
final_wf <-
nn_workflow %>%
finalize_workflow(bestTune) %>%
fit(data=train)
## Predict
ama_predictions <- predict(final_wf, new_data=test, type='class') %>%
rename(type=.pred_class) %>%
mutate(id = test$id) %>%
select(id, type)
vroom_write(x=ama_predictions, file="./nerualNet.csv", delim=",")
#stopCluster(cl)
